from fastapi import FastAPI
from pydantic import BaseModel
import torch

print("üöÄ Iniciando FastAPI...")

app = FastAPI()
service = None  # carregado apenas na primeira chamada

class PredictInput(BaseModel):
    mask: list  # √≠ndices dos n√≥s que voc√™ quer prever

class PredictOutput(BaseModel):
    predictions: list


@app.get("/")
def root():
    return {"status": "ok", "message": "API est√° rodando"}


@app.post("/predict", response_model=PredictOutput)
def predict(payload: PredictInput):
    global service

    # Carrega o servi√ßo e o grafo completo apenas na primeira chamada
    if service is None:
        print("üì¶ Carregando ModelService e grafo completo pela primeira vez...")

        from model_service import ModelService
        service = ModelService()

        # Carrega o grafo que o modelo realmente usa
        service.x_all = torch.load("model/x_all.pt")          # shape [N, 17]
        service.edge_index = torch.load("model/edge_index.pt") # shape [2, E]

        print("‚úÖ Grafo carregado.")

    try:
        # Converte m√°scara vinda da requisi√ß√£o
        mask_tensor = torch.tensor(payload.mask, dtype=torch.long)

        # Chama a predi√ß√£o real
        preds = service.predict(service.x_all, service.edge_index, mask_tensor)

        return PredictOutput(predictions=preds.tolist())

    except Exception as e:
        import traceback
        print("‚ùå Erro durante a predi√ß√£o:")
        traceback.print_exc()
        raise e
