from fastapi import FastAPI
from pydantic import BaseModel
import torch

print("游 Iniciando FastAPI...")

app = FastAPI()
service = None  # s칩 carrega quando necess치rio

class PredictInput(BaseModel):
    mask: list   # agora s칩 precisa disso

class PredictOutput(BaseModel):
    predictions: list

@app.get("/")
def root():
    return {"status": "ok", "message": "API est치 rodando"}

@app.post("/predict", response_model=PredictOutput)
def predict(payload: PredictInput):
    global service
    if service is None:
        print("游닍 Carregando ModelService pela primeira vez...")
        from model_service import ModelService
        service = ModelService()

        # Carrega o grafo completo j치 salvo
        service.x_all = torch.load("model/x_all.pt")
        service.edge_index = torch.load("model/edge_index.pt")

    try:
        mask_tensor = torch.tensor(payload.mask, dtype=torch.long)

        preds = service.predict(service.x_all, service.edge_index, mask_tensor)
        return PredictOutput(predictions=preds.tolist())

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise e
