# %% [code]
# %% [code] {"execution":{"iopub.status.busy":"2025-06-27T13:34:31.619179Z","iopub.execute_input":"2025-06-27T13:34:31.619508Z","iopub.status.idle":"2025-06-27T13:34:34.679744Z","shell.execute_reply.started":"2025-06-27T13:34:31.619481Z","shell.execute_reply":"2025-06-27T13:34:34.678915Z"},"jupyter":{"outputs_hidden":false}}
# EDA, pr√©-processamento e engenharia ‚Äî dados do MVP com automa√ß√£o e reprodutibilidade
# EDA, preprocesamiento e ingenier√≠a ‚Äî datos del MVP con automatizaci√≥n y reproducibilidad
# EDA, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–∏—è ‚Äî –¥–∞–Ω–Ω—ã–µ MVP —Å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å—é
# EDA, preprocessing and engineering ‚Äî MVP data with automation and reproducibility

import os
import re
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import joblib
from pandas.api.types import is_numeric_dtype
import warnings

# Ignorar warnings RuntimeWarning | Ignorar advertencias RuntimeWarning | –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è RuntimeWarning | Ignore RuntimeWarning warnings
warnings.filterwarnings('ignore', category=RuntimeWarning)

# Listar arquivos em /kaggle/input | Listar archivos en /kaggle/input | –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ /kaggle/input | List files in /kaggle/input
print("Arquivos dispon√≠veis em /kaggle/input:")
for root, _, files in os.walk('/kaggle/input'):
    for f in files:
        print(os.path.join(root, f))

# Ler CSV treino | Leer CSV entrenamiento | –ß—Ç–µ–Ω–∏–µ CSV –æ–±—É—á–µ–Ω–∏—è | Read train CSV
train = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')

# Ler CSV teste | Leer CSV prueba | –ß—Ç–µ–Ω–∏–µ CSV —Ç–µ—Å—Ç–∞ | Read test CSV
test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')

# Ler CSV submiss√£o | Leer CSV submission | –ß—Ç–µ–Ω–∏–µ CSV —Å–∞–±–º–∏—à–Ω | Read submission CSV
submission = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')

# Mostrar primeiras linhas treino | Mostrar primeras filas entrenamiento | –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ–±—É—á–µ–Ω–∏—è | Show first train rows
print("\nPrimeiras linhas do dataset de treino:\n", train.head())

# Info geral dataframe | Info general del dataframe | –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ | General dataframe info
print("\nInforma√ß√µes gerais do dataset:")
train.info()

# Contar valores ausentes | Contar valores faltantes | –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π | Count missing values
print("\nValores ausentes por coluna:\n", train.isnull().sum())

# Estat√≠sticas descritivas | Estad√≠sticas descriptivas | –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ | Descriptive statistics
print("\nEstat√≠sticas descritivas:\n", train.describe())

target_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']

# Estat√≠sticas vari√°veis alvo | Estad√≠sticas variables objetivo | –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö | Target variables statistics
print("\nEstat√≠sticas individuais das vari√°veis alvo:")
for c in target_cols:
    print(f"\n{c}:\n{train[c].describe()}")

# Matriz correla√ß√£o alvo | Matriz correlaci√≥n objetivo | –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Ü–µ–ª–µ–π | Target correlation matrix
print("\nMatriz de correla√ß√£o entre as vari√°veis alvo:\n", train[target_cols].corr())

# Contar SMILES √∫nicos | Contar SMILES √∫nicos | –ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö SMILES | Count unique SMILES
print("\nTotal de SMILES √∫nicos:", train['SMILES'].nunique())

# Encontrar SMILES duplicados | Encontrar duplicados SMILES | –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ SMILES | Find duplicate SMILES
duplicates = train[train.duplicated('SMILES')]
print(f"\nN√∫mero de SMILES duplicados: {len(duplicates)}")
if not duplicates.empty:
    print(duplicates.head())

# Criar coluna comprimento SMILES | Crear columna longitud SMILES | –°–æ–∑–¥–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É –¥–ª–∏–Ω—ã SMILES | Create SMILES length column
train['smiles_length'] = train['SMILES'].str.len()

# Estat√≠sticas comprimento SMILES | Estad√≠sticas longitud SMILES | –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã SMILES | SMILES length statistics
print("\nEstat√≠sticas do comprimento dos SMILES:\n", train['smiles_length'].describe())

# Mostrar 5 menores SMILES | Mostrar 5 SMILES m√°s cortos | –ü–æ–∫–∞–∑–∞—Ç—å 5 —Å–∞–º—ã—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö SMILES | Show 5 shortest SMILES
print("\n5 menores SMILES:\n", train.nsmallest(5, 'smiles_length')[['SMILES','smiles_length']])

# Mostrar 5 maiores SMILES | Mostrar 5 SMILES m√°s largos | –ü–æ–∫–∞–∑–∞—Ç—å 5 —Å–∞–º—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö SMILES | Show 5 longest SMILES
print("\n5 maiores SMILES:\n", train.nlargest(5, 'smiles_length')[['SMILES','smiles_length']])

# Extrair features SMILES | Extraer features SMILES | –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SMILES | Extract SMILES features
def smiles_robust_features(smiles):
    atom_counts = {
        'C': len(re.findall(r'C(?![a-z])', smiles)),
        'O': len(re.findall(r'O', smiles)),
        'N': len(re.findall(r'N(?![a-z])', smiles)),
        'S': len(re.findall(r'S(?![a-z])', smiles)),
        'F': len(re.findall(r'F', smiles)),
        'Cl': len(re.findall(r'Cl', smiles)),
        'Br': len(re.findall(r'Br', smiles)),
        'I': len(re.findall(r'I', smiles)),
        'P': len(re.findall(r'P', smiles)),
    }
    features = {
        'smiles_length': len(smiles),
        'num_branches': smiles.count('(') + smiles.count(')'),
        'num_double_bonds': smiles.count('='),
        'num_triple_bonds': smiles.count('#'),
        'num_ring_closures': len(re.findall(r'\d', smiles)),
        'num_aromatic_atoms': len(re.findall(r'[bcnops]', smiles)),
        'num_aliphatic_atoms': len(re.findall(r'[BCNOPSFHI]', smiles)),
        **atom_counts,
    }
    features['num_atoms_total'] = sum(atom_counts.values())
    features['num_atoms_unique'] = sum(v > 0 for v in atom_counts.values())
    return pd.Series(features)

# Aplicar features SMILES no treino | Aplicar features SMILES en entrenamiento | –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ SMILES –∫ –æ–±—É—á–µ–Ω–∏—é | Apply SMILES features to train
train = pd.concat([train, train['SMILES'].apply(smiles_robust_features)], axis=1)

# Preencher NA vari√°veis alvo | Rellenar NA variables objetivo | –ó–∞–ø–æ–ª–Ω–∏—Ç—å NA —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ | Fill NA target variables
train[target_cols] = train[target_cols].fillna(train[target_cols].mean())

exclude_cols = ['id', 'SMILES'] + target_cols

# Selecionar colunas num√©ricas | Seleccionar columnas num√©ricas | –í—ã–±–æ—Ä —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ | Select numeric columns
numerical_cols = [col for col in train.columns if col not in exclude_cols and is_numeric_dtype(train[col])]
print("\nColunas num√©ricas detectadas automaticamente (ordem preservada):\n", numerical_cols)

# Inicializar scaler | Inicializar scaler | –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å scaler | Initialize scaler
scaler = StandardScaler()

# Normalizar colunas num√©ricas | Normalizar columnas num√©ricas | –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ | Normalize numeric columns
train[numerical_cols] = scaler.fit_transform(train[numerical_cols])

# Salvar scaler | Guardar scaler | –°–æ—Ö—Ä–∞–Ω–∏—Ç—å scaler | Save scaler
joblib.dump(scaler, "scaler.pkl")

# Extrair features SMILES teste | Extraer features SMILES test | –ò–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏ SMILES —Ç–µ—Å—Ç–∞ | Extract SMILES features test
X_test_raw = test['SMILES'].apply(smiles_robust_features)

# Ajustar ordem colunas teste | Ajustar orden columnas test | –°–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —Ç–µ—Å—Ç–∞ | Align test columns order
X_test_raw = X_test_raw[numerical_cols]

# Carregar scaler salvo | Cargar scaler guardado | –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π scaler | Load saved scaler
scaler = joblib.load("scaler.pkl")

# Aplicar scaler no teste | Aplicar scaler en test | –ü—Ä–∏–º–µ–Ω–∏—Ç—å scaler –∫ —Ç–µ—Å—Ç—É | Apply scaler to test
X_test = scaler.transform(X_test_raw)

# Extrair ids teste | Extraer ids test | –ò–∑–≤–ª–µ—á—å id —Ç–µ—Å—Ç–∞ | Extract test ids
y_test = test['id'].values

# Conferir shape X_test | Revisar forma X_test | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä X_test | Check X_test shape
print("\nX_test shape:", X_test.shape)

# Conferir shape y_test | Revisar forma y_test | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä y_test | Check y_test shape
print("y_test shape:", y_test.shape)

# %% [code] {"execution":{"iopub.status.busy":"2025-06-27T13:34:34.681267Z","iopub.execute_input":"2025-06-27T13:34:34.681719Z","iopub.status.idle":"2025-06-27T13:41:41.743956Z","shell.execute_reply.started":"2025-06-27T13:34:34.681695Z","shell.execute_reply":"2025-06-27T13:41:41.742907Z"},"jupyter":{"outputs_hidden":false}}
# Modelo | Modelo | –ú–æ–¥–µ–ª—å | Model
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler

# Caminho de dados | Ruta de datos | –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º | Data path
train = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')
test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')
submission = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')

# Alvos | Objetivos | –¶–µ–ª–∏ | Targets
target_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']

# Extra√ß√£o simples de features dos SMILES | Extracci√≥n de caracter√≠sticas | –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ | Feature extraction
def smiles_robust_features(smiles):
    import re
    atom_counts = {
        'C': len(re.findall(r'C(?![a-z])', smiles)),
        'O': len(re.findall(r'O', smiles)),
        'N': len(re.findall(r'N(?![a-z])', smiles)),
        'S': len(re.findall(r'S(?![a-z])', smiles)),
        'F': len(re.findall(r'F', smiles)),
        'Cl': len(re.findall(r'Cl', smiles)),
        'Br': len(re.findall(r'Br', smiles)),
        'I': len(re.findall(r'I', smiles)),
        'P': len(re.findall(r'P', smiles)),
    }
    features = {
        'smiles_length': len(smiles),
        'num_branches': smiles.count('(') + smiles.count(')'),
        'num_double_bonds': smiles.count('='),
        'num_triple_bonds': smiles.count('#'),
        'num_ring_closures': len(re.findall(r'\d', smiles)),
        'num_aromatic_atoms': len(re.findall(r'[bcnops]', smiles)),
        'num_aliphatic_atoms': len(re.findall(r'[BCNOPSFHI]', smiles)),
        **atom_counts,
    }
    features['num_atoms_total'] = sum(atom_counts.values())
    features['num_atoms_unique'] = sum(v > 0 for v in atom_counts.values())
    return pd.Series(features)

# Aplicar features | Aplicar caracter√≠sticas | –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ | Apply features
train_feats = train['SMILES'].apply(smiles_robust_features)
test_feats = test['SMILES'].apply(smiles_robust_features)

# Preenchimento de alvos ausentes | Relleno de objetivos | –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ü–µ–ª–µ–π | Fill missing targets
train[target_cols] = train[target_cols].fillna(train[target_cols].mean())

# Colunas num√©ricas | Columnas num√©ricas | –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ | Numerical columns
numerical_cols = train_feats.columns.tolist()

# Normaliza√ß√£o | Normalizaci√≥n | –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è | Normalization
scaler = StandardScaler()
X_train = scaler.fit_transform(train_feats[numerical_cols])
X_test = scaler.transform(test_feats[numerical_cols])
X_all = np.vstack([X_train, X_test])

# Similaridade por cosseno | Similitud coseno | –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ | Cosine similarity
threshold = 0.8
sim_matrix = cosine_similarity(X_all)
edges = np.array(np.nonzero(np.triu(sim_matrix, k=1) > threshold))
edges = np.hstack([edges, edges[::-1]])  # bidirecional
edge_index = torch.tensor(edges, dtype=torch.long)

# GCN manual | GCN manual | –†—É—á–Ω–æ–π GCN | Manual GCN
class ManualGCNLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.linear = nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        row, col = edge_index
        deg = torch.bincount(row, minlength=x.size(0)).float()
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]
        agg = torch.zeros_like(x)
        agg.index_add_(0, row, x[col] * norm.unsqueeze(1))
        return self.linear(agg)

# Modelo simples GNN | GNN simple | –ü—Ä–æ—Å—Ç–∞—è GNN | Simple GNN
class SimpleGNN(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.gcn1 = ManualGCNLayer(in_channels, hidden_channels)
        self.relu = nn.ReLU()
        self.gcn2 = ManualGCNLayer(hidden_channels, hidden_channels)
        self.dropout = nn.Dropout(p=0.2)
        self.linear = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.gcn1(x, edge_index)
        x = self.relu(x)
        x = self.gcn2(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear(x)
        return x

# Dispositivo | Dispositivo | –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ | Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Tensores | Tensores | –¢–µ–Ω–∑–æ—Ä—ã | Tensors
x_all = torch.tensor(X_all, dtype=torch.float32).to(device)
y_train = train[target_cols].values
y_test = np.zeros((X_test.shape[0], len(target_cols)))
y_all = torch.tensor(np.vstack([y_train, y_test]), dtype=torch.float32).to(device)

# M√°scaras | M√°scaras | –ú–∞—Å–∫–∏ | Masks
n_train = X_train.shape[0]
train_mask = torch.zeros(x_all.size(0), dtype=torch.bool).to(device)
train_mask[:n_train] = True
test_mask = ~train_mask
edge_index = edge_index.to(device)

# Instanciar modelo | Instanciar modelo | –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è | Instantiate model
model = SimpleGNN(
    in_channels=x_all.shape[1],
    hidden_channels=64,
    out_channels=len(target_cols)
).to(device)

# Otimizador | Optimizador | –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä | Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

# Treinamento | Entrenamiento | –û–±—É—á–µ–Ω–∏–µ | Training
print("Iniciando treinamento da GNN...")  # PT

for epoch in range(1, 101):
    model.train()
    optimizer.zero_grad()
    out = model(x_all, edge_index)
    loss = criterion(out[train_mask], y_all[train_mask])
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0 or epoch == 1:
        print(f"Epoch {epoch:03d} | Loss: {loss.item():.4f}")

# wMAE | wMAE | wMAE | wMAE
def weighted_mae(y_true, y_pred):
    n_samples, n_targets = y_true.shape
    n_j = np.sum(~np.isnan(y_true), axis=0)
    R_j = np.nanmax(y_true, axis=0) - np.nanmin(y_true, axis=0)
    w_j = 1 / (np.sqrt(n_j) * R_j)
    w_j /= w_j.sum()
    mae_j = np.nanmean(np.abs(y_true - y_pred), axis=0)
    wmae = np.sum(w_j * mae_j)
    return wmae

# Avalia√ß√£o | Evaluaci√≥n | –û—Ü–µ–Ω–∫–∞ | Evaluation
model.eval()
with torch.no_grad():
    train_preds = model(x_all, edge_index)[train_mask].cpu().numpy()
    y_train_true = y_all[train_mask].cpu().numpy()

wmae_train = weighted_mae(y_train_true, train_preds)
print(f"\nWeighted MAE (wMAE) no conjunto de treino: {wmae_train:.6f}")

# Infer√™ncia | Inferencia | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ | Inference
with torch.no_grad():
    predictions = model(x_all, edge_index)[test_mask].cpu().numpy()

import torch
from pathlib import Path

# Criar pasta 'model' se n√£o existir
Path("model").mkdir(exist_ok=True)

# Salvar os pesos do modelo treinado
torch.save(model.state_dict(), "model/simple_gnn.pt")

from pathlib import Path
Path("model").mkdir(exist_ok=True)

# Salvar modelo
torch.save(model.state_dict(), "model/simple_gnn.pt")

# üü¢ Salvar grafo completo
torch.save(x_all.cpu(), "model/x_all.pt")
torch.save(edge_index.cpu(), "model/edge_index.pt")

print("Arquivos salvos:")
print(os.listdir("model"))

# Submiss√£o | Env√≠o | –°–∞–±–º–∏—à–Ω | Submission
submission = pd.DataFrame({'id': test['id']})
for i, col in enumerate(target_cols):
    submission[col] = predictions[:, i]

submission.to_csv("submission.csv", index=False)
print("\nArquivo de submiss√£o salvo: submission.csv")

# Pr√©via | Vista previa | –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä | Preview
print("\nPr√©via da submiss√£o:")
print(submission.head())
